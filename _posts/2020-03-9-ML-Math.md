# Cheat Sheet: Math for Machine Learning

### Motivation
I often find myself needing to look up various mathematical components of machine learning models, loss functions, activation functions, etc. This is a space where I hope to keep a quick reference for such things, and also to get into the nitty gritty of some math things that previously I've always just accepted at face value or simply glanced over.

### Loss functions
#### KL Divergence
Kullback-Leibler Divergence (also called *relative entropy*) is a measure comparing a probability distribution to another, ground-truth distribution. A value of 0 signifies that the two distributions are identical.
